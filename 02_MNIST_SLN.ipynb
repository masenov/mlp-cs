{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial is an introduction to the first coursework about multi-layer networks (also known as Multi-Layer Perceptrons - MLPs - or Deep Neural Networks - DNNs). Here, we will show how to build a single layer linear model (similar to the one from the previous lab) for MNIST digit classification using the provided code-base. \n",
    "\n",
    "The principal purpose of this introduction is to get you familiar with how to connect the code blocks (and what operations each of them implements) in order to set up an experiment that includes 1) building the model structure 2) optimising the model's parameters (weights) and 3) evaluating the model on test data. \n",
    "\n",
    "## For those affected by notebook kernel issues\n",
    "\n",
    "In case you are still having issues with running notebook kernels, have a look at [this note](https://github.com/CSTR-Edinburgh/mlpractical/blob/master/kernel_issue_fix.md) on the GitHub.\n",
    "\n",
    "## Virtual environments\n",
    "\n",
    "Before you proceed onwards, remember to activate your virtual environment:\n",
    "   * If you were in last week's Tuesday or Wednesday group type `activate_mlp` or `source ~/mlpractical/venv/bin/activate`\n",
    "   * If you were in the Monday group:\n",
    "      + and if you have chosen the **comfy** way type: `workon mlpractical`\n",
    "      + and if you have chosen the **generic** way, `source` your virutal environment using `source` and specyfing the path to the activate script (you need to localise it yourself, there were not any general recommendations w.r.t dir structure and people have installed it in different places, usually somewhere in the home directories. If you cannot easily find it by yourself, use something like: `find . -iname activate` ):\n",
    "\n",
    "## Syncing the git repository\n",
    "\n",
    "Look <a href=\"https://github.com/CSTR-Edinburgh/mlpractical/blob/master/gitFAQ.md\">here</a> for more details. But in short, we recommend to create a separate branch for the coursework, as follows:\n",
    "\n",
    "1. Enter the mlpractical directory `cd ~/mlpractical/repo-mlp`\n",
    "2. List the branches and check which is currently active by typing: `git checkout`\n",
    "3. If you are not in `master` branch, switch to it by typing: \n",
    "```\n",
    "git checkout master\n",
    " ```\n",
    "4. Then update the repository (note, assuming master does not have any conflicts), if there are some, have a look <a href=\"https://github.com/CSTR-Edinburgh/mlpractical/blob/master/gitFAQ.md\">here</a>\n",
    "```\n",
    "git pull\n",
    "```\n",
    "5. And now, create the new branch & swith to it by typing:\n",
    "```\n",
    "git checkout -b coursework1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Models\n",
    "\n",
    "Today, we shall build models which can have an arbitrary number of hidden layers.  Please have a look at the  diagram below, and the corresponding computations (which have an *exact* matrix form as expected by numpy, and row-wise orientation; note that $\\circ$ denotes an element-wise product). In the diagram, we briefly describe how each comptation relates to the code we have provided.\n",
    "\n",
    "![Making Predictions](res/code_scheme.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Structuring the model\n",
    "   * The model (for now) is allowed to have a sequence of layers, mapping inputs $\\mathbf{x}$ to outputs $\\mathbf{y}$. \n",
    "   * This operation is implemented as a special type of a layer in `mlp.layers.MLP` class. It keeps a sequence of other layers (of various typyes like Linear, Sigmoid, Softmax, etc.) as well as the internal state of a model for a mini-batch, that is, the intermediate data produced in *forward* and *backward* passes.\n",
    "2. Forward computation\n",
    "    * `mlp.layers.MLP` provides an `fprop()` method that iterates over defined layers propagates $\\mathbf{x}$ to $\\mathbf{y}$. \n",
    "    * Each layer (look at `mlp.layers.Linear` attached below) also implements an `fprop()` method, which performs an atomic, for the given layer, operation. Most often, for the $i$-th layer, we want to obtain a linear transform $\\mathbf a^i$ of the inputs, and apply some non-linear transfer function $f^i(\\mathbf a^i)$ to produce the output $\\mathbf h^i$. Note, in general each layer may implement different activation functions $f^i()$, however for now we will use only `sigmoid` and `softmax`\n",
    "3. Backward computation\n",
    "   * Similarly, `mlp.layers.MLP` also implements a `bprop()` function, to back-propagate the errors from the top to the bottom layer. This class also keeps the back-propagated statistics ($\\delta$) to be used later when computing the gradients with respect to the parameters.\n",
    "   * This functionality is also re-implemented by particular layers (again, have a look at the `bprop` function of `mlp.layers.Linear`). `bprop()`  returns both $\\delta$ (needed to update the parameters) but also back-progapates the gradient down to the inputs. Also note, that depending on whether the layer is the top or not (i.e. if it deals directly with the cost function or not) some simplifications may apply ( as with cross-entropy and softmax). That's why when implementing a new type of layer that may be used as an output layer one also need to specify the implementation of `bprop_cost()`.\n",
    "4. Learning the model\n",
    "   * The actual evaluation of the cost as well as the *forward* and *backward* passes may be found in the `train_epoch()` method of `mlp.optimisers.SGDOptimiser`\n",
    "   * This function also calls the `pgrads()` method on each layer, that given activations and deltas, returns the list of the gradients of the cost with respect to the model parameters, i.e. $\\frac{\\partial{\\mathbf{E}}}{\\partial{\\mathbf{W^i}}}$ and  $\\frac{\\partial{\\mathbf{E}}}{\\partial{\\mathbf{b}^i}}$ at the above diagram (look at an example implementation in `mlp.layers.Linear`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-43a59954cdd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# %load -s Linear mlp/layers.py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# DO NOT RUN THIS CELL (AS YOU WILL GET ERRORS), IT WAS JUST LOADED TO VISUALISE ABOVE COMMENTS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     def __init__(self, idim, odim,\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Layer' is not defined"
     ]
    }
   ],
   "source": [
    "# %load -s Linear mlp/layers.py\n",
    "# DO NOT RUN THIS CELL (AS YOU WILL GET ERRORS), IT WAS JUST LOADED TO VISUALISE ABOVE COMMENTS\n",
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, idim, odim,\n",
    "                 rng=None,\n",
    "                 irange=0.1):\n",
    "\n",
    "        super(Linear, self).__init__(rng=rng)\n",
    "\n",
    "        self.idim = idim\n",
    "        self.odim = odim\n",
    "\n",
    "        self.W = self.rng.uniform(\n",
    "            -irange, irange,\n",
    "            (self.idim, self.odim))\n",
    "\n",
    "        self.b = numpy.zeros((self.odim,), dtype=numpy.float32)\n",
    "\n",
    "    def fprop(self, inputs):\n",
    "        \"\"\"\n",
    "        Implements a forward propagation through the i-th layer, that is\n",
    "        some form of:\n",
    "           a^i = xW^i + b^i\n",
    "           h^i = f^i(a^i)\n",
    "        with f^i, W^i, b^i denoting a non-linearity, weight matrix and\n",
    "        biases of this (i-th) layer, respectively and x denoting inputs.\n",
    "\n",
    "        :param inputs: matrix of features (x) or the output of the previous layer h^{i-1}\n",
    "        :return: h^i, matrix of transformed by layer features\n",
    "        \"\"\"\n",
    "        a = numpy.dot(inputs, self.W) + self.b\n",
    "        # here f() is an identity function, so just return a linear transformation\n",
    "        return a\n",
    "\n",
    "    def bprop(self, h, igrads):\n",
    "        \"\"\"\n",
    "        Implements a backward propagation through the layer, that is, given\n",
    "        h^i denotes the output of the layer and x^i the input, we compute:\n",
    "        dh^i/dx^i which by chain rule is dh^i/da^i da^i/dx^i\n",
    "        x^i could be either features (x) or the output of the lower layer h^{i-1}\n",
    "        :param h: it's an activation produced in forward pass\n",
    "        :param igrads, error signal (or gradient) flowing to the layer, note,\n",
    "               this in general case does not corresponds to 'deltas' used to update\n",
    "               the layer's parameters, to get deltas ones need to multiply it with\n",
    "               the dh^i/da^i derivative\n",
    "        :return: a tuple (deltas, ograds) where:\n",
    "               deltas = igrads * dh^i/da^i\n",
    "               ograds = deltas \\times da^i/dx^i\n",
    "        \"\"\"\n",
    "\n",
    "        # since df^i/da^i = 1 (f is assumed identity function),\n",
    "        # deltas are in fact the same as igrads\n",
    "        ograds = numpy.dot(igrads, self.W.T)\n",
    "        return igrads, ograds\n",
    "\n",
    "    def bprop_cost(self, h, igrads, cost):\n",
    "        \"\"\"\n",
    "        Implements a backward propagation in case the layer directly\n",
    "        deals with the optimised cost (i.e. the top layer)\n",
    "        By default, method should implement a bprop for default cost, that is\n",
    "        the one that is natural to the layer's output, i.e.:\n",
    "        here we implement linear -> mse scenario\n",
    "        :param h: it's an activation produced in forward pass\n",
    "        :param igrads, error signal (or gradient) flowing to the layer, note,\n",
    "               this in general case does not corresponds to 'deltas' used to update\n",
    "               the layer's parameters, to get deltas ones need to multiply it with\n",
    "               the dh^i/da^i derivative\n",
    "        :param cost, mlp.costs.Cost instance defining the used cost\n",
    "        :return: a tuple (deltas, ograds) where:\n",
    "               deltas = igrads * dh^i/da^i\n",
    "               ograds = deltas \\times da^i/dx^i\n",
    "        \"\"\"\n",
    "\n",
    "        if cost is None or cost.get_name() == 'mse':\n",
    "            # for linear layer and mean square error cost,\n",
    "            # cost back-prop is the same as standard back-prop\n",
    "            return self.bprop(h, igrads)\n",
    "        else:\n",
    "            raise NotImplementedError('Linear.bprop_cost method not implemented '\n",
    "                                      'for the %s cost' % cost.get_name())\n",
    "\n",
    "    def pgrads(self, inputs, deltas):\n",
    "        \"\"\"\n",
    "        Return gradients w.r.t parameters\n",
    "\n",
    "        :param inputs, input to the i-th layer\n",
    "        :param deltas, deltas computed in bprop stage up to -ith layer\n",
    "        :return list of grads w.r.t parameters dE/dW and dE/db in *exactly*\n",
    "                the same order as the params are returned by get_params()\n",
    "\n",
    "        Note: deltas here contain the whole chain rule leading\n",
    "        from the cost up to the the i-th layer, i.e.\n",
    "        dE/dy^L dy^L/da^L da^L/dh^{L-1} dh^{L-1}/da^{L-1} ... dh^{i}/da^{i}\n",
    "        and here we are just asking about\n",
    "          1) da^i/dW^i and 2) da^i/db^i\n",
    "        since W and b are only layer's parameters\n",
    "        \"\"\"\n",
    "\n",
    "        grad_W = numpy.dot(inputs.T, deltas)\n",
    "        grad_b = numpy.sum(deltas, axis=0)\n",
    "\n",
    "        return [grad_W, grad_b]\n",
    "\n",
    "    def get_params(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def set_params(self, params):\n",
    "        #we do not make checks here, but the order on the list\n",
    "        #is assumed to be exactly the same as get_params() returns\n",
    "        self.W = params[0]\n",
    "        self.b = params[1]\n",
    "\n",
    "    def get_name(self):\n",
    "        return 'linear'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Experiment with linear models and MNIST\n",
    "\n",
    "The below snippet demonstrates how to use the code we have provided for the coursework 1. Get familiar with it, as from now on we will use till the end of the course, including the 2nd coursework.\n",
    "\n",
    "It should be straightforward to extend the following code to more complex models, like stack more layers, change the cost, the optimiser, learning rate schedules, etc.. But **ask** in case something is not clear.\n",
    "\n",
    "In this particular example, we use the following components:\n",
    "  *  One layer mapping data-points ($\\mathbf x$) straight to 10 digits classes represented as 10 (linear) outputs ($\\mathbf y$). This operation is implemented as a linear layer in `mlp.layers.Linear`. Get familiar with this class (read the comments, etc.) as it is going to be a building block for the coursework.\n",
    "  * One can stack as many different layers as required through the container `mlp.layers.MLP`\n",
    "  * As an objective here we use the Mean Square Error cost defined in `mlp.costs.MSECost`\n",
    "  * Our *Stochastic Gradient Descent* optimiser can be found in `mlp.optimisers.SGDOptimiser`. Its parent `mlp.optimisers.Optimiser` implements validation functionality (and an interface in case one need to implement a different optimiser)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (mse) for random model is 1.866. Accuracy is 9.42%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (mse) for random model is 1.866. Accuracy is 8.25%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (mse) is 0.441. Accuracy is 60.32%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (mse) is 0.304. Accuracy is 76.56%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 1 seconds. Training speed 62165 pps. Validation speed 114209 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (mse) is 0.289. Accuracy is 76.93%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (mse) is 0.260. Accuracy is 80.93%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 1 seconds. Training speed 57817 pps. Validation speed 126582 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (mse) is 0.261. Accuracy is 79.90%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (mse) is 0.241. Accuracy is 82.94%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 1 seconds. Training speed 60541 pps. Validation speed 197992 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (mse) is 0.246. Accuracy is 81.49%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (mse) is 0.230. Accuracy is 83.50%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 1 seconds. Training speed 57823 pps. Validation speed 168651 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (mse) is 0.237. Accuracy is 82.25%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (mse) is 0.223. Accuracy is 84.27%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 1 seconds. Training speed 63290 pps. Validation speed 155960 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (mse) is 0.231. Accuracy is 82.82%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (mse) is 0.217. Accuracy is 84.80%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 1 seconds. Training speed 66004 pps. Validation speed 172360 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (mse) is 0.226. Accuracy is 83.30%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (mse) is 0.213. Accuracy is 84.86%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 1 seconds. Training speed 63519 pps. Validation speed 182672 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (mse) is 0.222. Accuracy is 83.51%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (mse) is 0.210. Accuracy is 85.15%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 1 seconds. Training speed 66945 pps. Validation speed 163959 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (mse) is 0.219. Accuracy is 83.64%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (mse) is 0.208. Accuracy is 85.56%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 1 seconds. Training speed 67665 pps. Validation speed 149249 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (mse) is 0.217. Accuracy is 83.93%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (mse) is 0.205. Accuracy is 85.43%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 1 seconds. Training speed 65076 pps. Validation speed 164096 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (mse) is 0.215. Accuracy is 84.05%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (mse) is 0.203. Accuracy is 85.81%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 1 seconds. Training speed 65975 pps. Validation speed 151653 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (mse) is 0.213. Accuracy is 84.23%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (mse) is 0.202. Accuracy is 86.15%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 1 seconds. Training speed 63957 pps. Validation speed 165314 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (mse) is 0.211. Accuracy is 84.13%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (mse) is 0.200. Accuracy is 85.95%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 1 seconds. Training speed 64917 pps. Validation speed 170311 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (mse) is 0.210. Accuracy is 84.37%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (mse) is 0.200. Accuracy is 86.04%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 1 seconds. Training speed 67006 pps. Validation speed 167068 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (mse) is 0.209. Accuracy is 84.44%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (mse) is 0.199. Accuracy is 85.80%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 1 seconds. Training speed 61566 pps. Validation speed 158331 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (mse) is 0.208. Accuracy is 84.52%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (mse) is 0.198. Accuracy is 86.18%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 1 seconds. Training speed 66632 pps. Validation speed 179688 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (mse) is 0.207. Accuracy is 84.66%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (mse) is 0.197. Accuracy is 86.00%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 1 seconds. Training speed 48865 pps. Validation speed 127188 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (mse) is 0.206. Accuracy is 84.62%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (mse) is 0.196. Accuracy is 86.33%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 1 seconds. Training speed 43960 pps. Validation speed 109281 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (mse) is 0.205. Accuracy is 84.73%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (mse) is 0.196. Accuracy is 86.05%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 1 seconds. Training speed 46844 pps. Validation speed 182266 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (mse) is 0.204. Accuracy is 84.70%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (mse) is 0.195. Accuracy is 85.98%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 1 seconds. Training speed 67006 pps. Validation speed 166022 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 85.41 % (cost is 0.201)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from mlp.layers import MLP, Linear #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "from mlp.dataset import MNISTDataProvider #import data provider\n",
    "from mlp.costs import MSECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "# define the model structure, here just one linear layer\n",
    "# and mean square error cost\n",
    "cost = MSECost()\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(Linear(idim=784, odim=10, rng=rng))\n",
    "#one can stack more layers here\n",
    "\n",
    "# define the optimiser, here stochasitc gradient descent\n",
    "# with fixed learning rate and max_epochs as stopping criterion\n",
    "lr_scheduler = LearningRateFixed(learning_rate=0.01, max_epochs=20)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Initialising data providers...')\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=-10, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=100, max_num_batches=-10, randomize=False)\n",
    "\n",
    "logger.info('Training started...')\n",
    "optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "logger.info('Testing the model on test set:')\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=100, max_num_batches=-10, randomize=False)\n",
    "cost, accuracy = optimiser.validate(model, test_dp)\n",
    "logger.info('MNIST test set accuracy is %.2f %% (cost is %.3f)'%(accuracy*100., cost))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Modify the above code by adding an intemediate linear layer of size 200 hidden units between input and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
